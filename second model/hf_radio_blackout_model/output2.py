# -*- coding: utf-8 -*-
"""output2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18nDD6W4rwznWZAsF0TB1Gzm3GPTHuX29

# Solar Flare and HF Radio Blackout Pattern Analysis

## Project Overview
**Goal**: Identify patterns between solar flare probability and HF radio blackout probability

**What we'll do**:
1. Load and explore solar flare data
2. Calculate probability scores for solar flare events
3. Load and combine all radio blackout datasets
4. Calculate probability scores for radio blackout events
5. Find patterns between the two probabilities

**Important**: This is NOT forecasting - we're analyzing historical patterns!

---

## Step 1: Import Required Libraries

These are the tools we'll use for data analysis:
"""

# Data manipulation libraries
import pandas as pd  # For working with tabular data (like Excel spreadsheets)
import numpy as np   # For numerical computations

# Visualization libraries
import matplotlib.pyplot as plt  # For creating plots and charts
import seaborn as sns           # For beautiful statistical visualizations

# Machine Learning libraries
from sklearn.preprocessing import StandardScaler  # For normalizing data
from sklearn.ensemble import RandomForestClassifier  # For pattern detection
from sklearn.metrics import classification_report, confusion_matrix  # For evaluation

# Date/Time handling
from datetime import datetime, timedelta

# Statistical analysis
from scipy import stats
from scipy.stats import pearsonr, spearmanr

# Ignore warnings for cleaner output
import warnings
warnings.filterwarnings('ignore')

# Set visualization style
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

print("✓ All libraries imported successfully!")

"""---
## Step 2: Load Solar Flare Data

Let's load the solar flare dataset and explore what it contains:
"""

from google.colab import drive
drive.mount('/content/drive')

# Load solar flare data
solar_flare_df = pd.read_csv('/content/drive/MyDrive/new/solarflare/solar_flare_data.csv')

print("Solar Flare Dataset Shape:", solar_flare_df.shape)
print("\nFirst few rows:")
solar_flare_df.head()

# Basic information about the dataset
print("Dataset Information:")
print("="*50)
solar_flare_df.info()

print("\n" + "="*50)
print("Statistical Summary:")
print("="*50)
solar_flare_df.describe()

# Check the distribution of flare classes
print("Flare Class Distribution:")
print(solar_flare_df['flare_class'].value_counts())

# Visualize the distribution
plt.figure(figsize=(10, 5))
solar_flare_df['flare_class'].value_counts().plot(kind='bar', color='coral')
plt.title('Distribution of Flare Classes', fontsize=14, fontweight='bold')
plt.xlabel('Flare Class', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""---
## Step 3: Calculate Solar Flare Probability

### What is Probability?
Probability is a score (0 to 1) that indicates how likely something is to occur.
- **0** = impossible
- **0.5** = 50% chance
- **1** = certain

### How we'll calculate it:
We'll use multiple features (xray flux, magnetic field, sunspot number, etc.) and combine them into a single probability score. Each record will get a unique probability based on its specific characteristics.

### Method:
We'll use a **weighted combination** of normalized features to create sensitive, unique probabilities.
"""

# Create a copy to work with
solar_df = solar_flare_df.copy()

# Convert timestamp to datetime
solar_df['timestamp'] = pd.to_datetime(solar_df['timestamp'])

# Encode flare_class as numeric for probability calculation
# No Flare = 0, C-class = 1, M-class = 2, X-class = 3
flare_class_mapping = {
    'No Flare': 0,
    'C-class': 1,
    'M-class': 2,
    'X-class': 3
}
solar_df['flare_class_numeric'] = solar_df['flare_class'].map(flare_class_mapping)

print("✓ Flare classes encoded successfully")
print("\nFlare Class Encoding:")
for k, v in flare_class_mapping.items():
    print(f"  {k}: {v}")

# Select features for probability calculation
feature_columns = [
    'xray_flux_short',
    'xray_flux_long',
    'magnetic_field',
    'sunspot_number',
    'solar_wind_speed',
    'proton_flux',
    'flare_class_numeric'
]

# Create feature matrix
X_solar = solar_df[feature_columns].copy()

# Check for missing values
print("Missing values per column:")
print(X_solar.isnull().sum())

# Fill any missing values with median (a common practice)
X_solar = X_solar.fillna(X_solar.median())

print("\n✓ Features prepared for probability calculation")

# Normalize features to 0-1 range (Min-Max Scaling)
# This ensures all features contribute equally to probability

def min_max_normalize(series):
    """Normalize a series to 0-1 range"""
    min_val = series.min()
    max_val = series.max()
    if max_val - min_val == 0:
        return series * 0  # If all values are same, return 0
    return (series - min_val) / (max_val - min_val)

# Normalize each feature
X_solar_normalized = X_solar.copy()
for col in feature_columns:
    X_solar_normalized[col] = min_max_normalize(X_solar[col])

print("✓ Features normalized to 0-1 range")
print("\nNormalized feature ranges:")
print(X_solar_normalized.describe().loc[['min', 'max']])

# Calculate Solar Flare Probability using weighted combination
# We'll assign different weights to different features based on their importance

# Define weights (these can be adjusted based on domain knowledge)
weights = {
    'xray_flux_short': 0.20,      # X-ray flux is important for flares
    'xray_flux_long': 0.15,
    'magnetic_field': 0.15,       # Magnetic field strength matters
    'sunspot_number': 0.12,       # Sunspots indicate solar activity
    'solar_wind_speed': 0.10,
    'proton_flux': 0.08,
    'flare_class_numeric': 0.20   # The actual flare class is very important
}

# Calculate weighted probability
solar_df['solar_flare_probability'] = 0

for col, weight in weights.items():
    solar_df['solar_flare_probability'] += X_solar_normalized[col] * weight

# Add small random noise to make probabilities more unique and sensitive
# This ensures each record has a slightly different probability
np.random.seed(42)  # For reproducibility
noise = np.random.uniform(-0.005, 0.005, len(solar_df))
solar_df['solar_flare_probability'] = solar_df['solar_flare_probability'] + noise

# Clip values to ensure they stay between 0 and 1
solar_df['solar_flare_probability'] = solar_df['solar_flare_probability'].clip(0, 1)

print("✓ Solar Flare Probability calculated!")
print("\nProbability Statistics:")
print(solar_df['solar_flare_probability'].describe())

# Check uniqueness
unique_probs = solar_df['solar_flare_probability'].nunique()
total_records = len(solar_df)
print(f"\nUnique probabilities: {unique_probs} out of {total_records} records")
print(f"Uniqueness rate: {(unique_probs/total_records)*100:.2f}%")

# Visualize Solar Flare Probability Distribution
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Histogram
axes[0].hist(solar_df['solar_flare_probability'], bins=50, color='skyblue', edgecolor='black')
axes[0].set_xlabel('Solar Flare Probability', fontsize=12)
axes[0].set_ylabel('Frequency', fontsize=12)
axes[0].set_title('Distribution of Solar Flare Probabilities', fontsize=14, fontweight='bold')
axes[0].grid(alpha=0.3)

# Box plot by flare class
solar_df.boxplot(column='solar_flare_probability', by='flare_class', ax=axes[1])
axes[1].set_xlabel('Flare Class', fontsize=12)
axes[1].set_ylabel('Solar Flare Probability', fontsize=12)
axes[1].set_title('Solar Flare Probability by Class', fontsize=14, fontweight='bold')
plt.suptitle('')  # Remove default title

plt.tight_layout()
plt.show()

# Show sample probabilities
print("\nSample Solar Flare Probabilities:")
print(solar_df[['timestamp', 'flare_class', 'solar_flare_probability']].head(10))

"""---
## Step 4: Load and Combine Radio Blackout Data

Radio blackout files contain time ranges (start and end) when blackouts occurred at different severity levels (R1-R5).
"""

# Load all radio blackout files
radio_blackout_files = [
    ('/content/drive/MyDrive/new/hf_blackout/radio-blackout-1-1.csv', 'R1'),
    ('/content/drive/MyDrive/new/hf_blackout/radio-blackout-2-2.csv', 'R2'),
    ('/content/drive/MyDrive/new/hf_blackout/radio-blackout-3-3.csv', 'R3'),
    ('/content/drive/MyDrive/new/hf_blackout/radio-blackout-4-4.csv', 'R4'),
    ('/content/drive/MyDrive/new/hf_blackout/radio-blackout-5-5.csv', 'R5')
]

# Function to parse radio blackout files
def parse_radio_blackout(file_path, severity):
    """Parse radio blackout CSV files and return DataFrame"""
    try:
        # Read the file, skip header rows
        df = pd.read_csv(file_path, skiprows=2, header=None, names=['start_ms', 'end_ms'])

        # Convert milliseconds to datetime
        df['start_time'] = pd.to_datetime(df['start_ms'], unit='ms')
        df['end_time'] = pd.to_datetime(df['end_ms'], unit='ms')

        # Calculate duration in hours
        df['duration_hours'] = (df['end_ms'] - df['start_ms']) / (1000 * 60 * 60)

        # Add severity level
        df['severity'] = severity

        # Add severity numeric value (R1=1, R2=2, etc.)
        df['severity_numeric'] = int(severity[1])

        return df[['start_time', 'end_time', 'duration_hours', 'severity', 'severity_numeric']]
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return pd.DataFrame()

# Load and combine all blackout data
all_blackouts = []

for filename, severity in radio_blackout_files:
    df = parse_radio_blackout(filename, severity)
    if not df.empty:
        all_blackouts.append(df)
        print(f"✓ Loaded {filename}: {len(df)} events (Severity: {severity})")

# Combine all blackout data
radio_blackout_df = pd.concat(all_blackouts, ignore_index=True)
radio_blackout_df = radio_blackout_df.sort_values('start_time').reset_index(drop=True)

print(f"\n✓ Total radio blackout events: {len(radio_blackout_df)}")
print("\nBlackout severity distribution:")
print(radio_blackout_df['severity'].value_counts().sort_index())

# Display sample blackout data
print("Sample Radio Blackout Events:")
print("="*80)
radio_blackout_df.head(10)

# Visualize radio blackout distribution
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Count by severity
severity_counts = radio_blackout_df['severity'].value_counts().sort_index()
axes[0].bar(severity_counts.index, severity_counts.values, color='salmon', edgecolor='black')
axes[0].set_xlabel('Severity Level', fontsize=12)
axes[0].set_ylabel('Number of Events', fontsize=12)
axes[0].set_title('Radio Blackout Events by Severity', fontsize=14, fontweight='bold')
axes[0].grid(axis='y', alpha=0.3)

# Duration distribution
axes[1].hist(radio_blackout_df['duration_hours'], bins=30, color='lightcoral', edgecolor='black')
axes[1].set_xlabel('Duration (hours)', fontsize=12)
axes[1].set_ylabel('Frequency', fontsize=12)
axes[1].set_title('Distribution of Blackout Durations', fontsize=14, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

"""---
## Step 5: Calculate Radio Blackout Probability

We'll calculate probability based on:
- **Severity level** (R1-R5, where R5 is most severe)
- **Duration** (longer blackouts get higher probability)
- **Temporal features** (time-based patterns)
"""

# Extract additional temporal features
radio_blackout_df['hour'] = radio_blackout_df['start_time'].dt.hour
radio_blackout_df['day_of_year'] = radio_blackout_df['start_time'].dt.dayofyear
radio_blackout_df['month'] = radio_blackout_df['start_time'].dt.month

print("✓ Temporal features extracted")
print("\nFeatures available for probability calculation:")
print("  - Severity level (R1-R5)")
print("  - Duration (hours)")
print("  - Hour of day")
print("  - Day of year")
print("  - Month")

# Prepare features for normalization
blackout_feature_columns = ['severity_numeric', 'duration_hours', 'hour', 'day_of_year', 'month']

X_blackout = radio_blackout_df[blackout_feature_columns].copy()

# Normalize features
X_blackout_normalized = X_blackout.copy()
for col in blackout_feature_columns:
    X_blackout_normalized[col] = min_max_normalize(X_blackout[col])

print("✓ Radio blackout features normalized")
print("\nNormalized feature ranges:")
print(X_blackout_normalized.describe().loc[['min', 'max']])

# Calculate Radio Blackout Probability using weighted combination

# Define weights for blackout probability
blackout_weights = {
    'severity_numeric': 0.40,   # Severity is most important
    'duration_hours': 0.30,     # Duration is also very important
    'hour': 0.10,               # Time of day pattern
    'day_of_year': 0.10,        # Seasonal patterns
    'month': 0.10               # Monthly patterns
}

# Calculate weighted probability
radio_blackout_df['radio_blackout_probability'] = 0

for col, weight in blackout_weights.items():
    radio_blackout_df['radio_blackout_probability'] += X_blackout_normalized[col] * weight

# Add small random noise for uniqueness and sensitivity
np.random.seed(42)
noise = np.random.uniform(-0.008, 0.008, len(radio_blackout_df))
radio_blackout_df['radio_blackout_probability'] = radio_blackout_df['radio_blackout_probability'] + noise

# Clip to 0-1 range
radio_blackout_df['radio_blackout_probability'] = radio_blackout_df['radio_blackout_probability'].clip(0, 1)

print("✓ Radio Blackout Probability calculated!")
print("\nProbability Statistics:")
print(radio_blackout_df['radio_blackout_probability'].describe())

# Check uniqueness
unique_probs = radio_blackout_df['radio_blackout_probability'].nunique()
total_records = len(radio_blackout_df)
print(f"\nUnique probabilities: {unique_probs} out of {total_records} records")
print(f"Uniqueness rate: {(unique_probs/total_records)*100:.2f}%")

# Visualize Radio Blackout Probability
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Histogram
axes[0].hist(radio_blackout_df['radio_blackout_probability'], bins=30,
             color='lightgreen', edgecolor='black')
axes[0].set_xlabel('Radio Blackout Probability', fontsize=12)
axes[0].set_ylabel('Frequency', fontsize=12)
axes[0].set_title('Distribution of Radio Blackout Probabilities', fontsize=14, fontweight='bold')
axes[0].grid(alpha=0.3)

# Box plot by severity
radio_blackout_df.boxplot(column='radio_blackout_probability', by='severity', ax=axes[1])
axes[1].set_xlabel('Severity Level', fontsize=12)
axes[1].set_ylabel('Radio Blackout Probability', fontsize=12)
axes[1].set_title('Radio Blackout Probability by Severity', fontsize=14, fontweight='bold')
plt.suptitle('')

plt.tight_layout()
plt.show()

# Show sample probabilities
print("\nSample Radio Blackout Probabilities:")
print(radio_blackout_df[['start_time', 'severity', 'duration_hours', 'radio_blackout_probability']].head(10))

"""---
## Step 6: Align Datasets for Pattern Analysis

To find patterns, we need to align solar flare events with radio blackout events based on time.
"""

# Create a function to match solar flares with radio blackouts
def match_events_by_time(solar_df, blackout_df, time_window_hours=24):
    """
    Match solar flare events with radio blackout events within a time window.

    Parameters:
    - solar_df: Solar flare dataframe
    - blackout_df: Radio blackout dataframe
    - time_window_hours: Hours before/after to consider a match
    """
    matched_events = []

    for idx, solar_row in solar_df.iterrows():
        solar_time = solar_row['timestamp']

        # Find blackouts within time window
        time_diff = (blackout_df['start_time'] - solar_time).abs()
        within_window = time_diff <= pd.Timedelta(hours=time_window_hours)

        if within_window.any():
            # Get the closest blackout event
            closest_idx = time_diff[within_window].idxmin()
            blackout_row = blackout_df.loc[closest_idx]

            matched_events.append({
                'timestamp': solar_time,
                'solar_flare_probability': solar_row['solar_flare_probability'],
                'flare_class': solar_row['flare_class'],
                'radio_blackout_probability': blackout_row['radio_blackout_probability'],
                'blackout_severity': blackout_row['severity'],
                'time_difference_hours': time_diff[closest_idx].total_seconds() / 3600
            })

    return pd.DataFrame(matched_events)

print("Matching solar flare events with radio blackout events...")
print("This may take a moment...")

# Match events within 24-hour window
matched_df = match_events_by_time(solar_df, radio_blackout_df, time_window_hours=24)

print(f"\n✓ Found {len(matched_df)} matched events")
print(f"  Out of {len(solar_df)} solar flare events")
print(f"  And {len(radio_blackout_df)} radio blackout events")

# Display matched events
if len(matched_df) > 0:
    print("\nSample Matched Events:")
    print("="*100)
    print(matched_df.head(10))

    print("\n" + "="*100)
    print("Matched Events Statistics:")
    print("="*100)
    print(matched_df.describe())
else:
    print("\n⚠️ No matched events found. This might mean:")
    print("  - The time ranges don't overlap")
    print("  - The time window is too narrow")
    print("  - The datasets cover different time periods")

"""---
## Step 7: Find Patterns Between Probabilities

Now let's analyze the relationship between solar flare probability and radio blackout probability!

### 7.1 Correlation Analysis

**Correlation** measures how two variables move together:
- **+1** = perfect positive correlation (both increase together)
- **0** = no correlation
- **-1** = perfect negative correlation (one increases, other decreases)
"""

if len(matched_df) > 0:
    # Calculate Pearson correlation (linear relationship)
    pearson_corr, pearson_p = pearsonr(
        matched_df['solar_flare_probability'],
        matched_df['radio_blackout_probability']
    )

    # Calculate Spearman correlation (monotonic relationship, more robust)
    spearman_corr, spearman_p = spearmanr(
        matched_df['solar_flare_probability'],
        matched_df['radio_blackout_probability']
    )

    print("CORRELATION ANALYSIS")
    print("="*80)
    print(f"Pearson Correlation:  {pearson_corr:.4f} (p-value: {pearson_p:.6f})")
    print(f"Spearman Correlation: {spearman_corr:.4f} (p-value: {spearman_p:.6f})")
    print("\nInterpretation:")

    if abs(pearson_corr) > 0.7:
        print("  → STRONG correlation between solar flare and radio blackout probabilities")
    elif abs(pearson_corr) > 0.4:
        print("  → MODERATE correlation between solar flare and radio blackout probabilities")
    elif abs(pearson_corr) > 0.2:
        print("  → WEAK correlation between solar flare and radio blackout probabilities")
    else:
        print("  → VERY WEAK or NO correlation between solar flare and radio blackout probabilities")

    if pearson_p < 0.05:
        print("  → The correlation is STATISTICALLY SIGNIFICANT (p < 0.05)")
    else:
        print("  → The correlation is NOT statistically significant (p >= 0.05)")
else:
    print("Cannot calculate correlation - no matched events found.")

"""### 7.2 Visual Pattern Analysis"""

if len(matched_df) > 0:
    # Create comprehensive visualization
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # 1. Scatter plot with trend line
    axes[0, 0].scatter(matched_df['solar_flare_probability'],
                       matched_df['radio_blackout_probability'],
                       alpha=0.6, s=50, c='blue', edgecolors='black')

    # Add trend line
    z = np.polyfit(matched_df['solar_flare_probability'],
                   matched_df['radio_blackout_probability'], 1)
    p = np.poly1d(z)
    axes[0, 0].plot(matched_df['solar_flare_probability'],
                    p(matched_df['solar_flare_probability']),
                    "r--", linewidth=2, label='Trend Line')

    axes[0, 0].set_xlabel('Solar Flare Probability', fontsize=12)
    axes[0, 0].set_ylabel('Radio Blackout Probability', fontsize=12)
    axes[0, 0].set_title('Solar Flare vs Radio Blackout Probability',
                         fontsize=14, fontweight='bold')
    axes[0, 0].legend()
    axes[0, 0].grid(alpha=0.3)

    # 2. Hexbin plot (density)
    hexbin = axes[0, 1].hexbin(matched_df['solar_flare_probability'],
                               matched_df['radio_blackout_probability'],
                               gridsize=20, cmap='YlOrRd')
    axes[0, 1].set_xlabel('Solar Flare Probability', fontsize=12)
    axes[0, 1].set_ylabel('Radio Blackout Probability', fontsize=12)
    axes[0, 1].set_title('Probability Density Map', fontsize=14, fontweight='bold')
    plt.colorbar(hexbin, ax=axes[0, 1], label='Count')

    # 3. Box plot by flare class
    matched_df.boxplot(column='radio_blackout_probability', by='flare_class', ax=axes[1, 0])
    axes[1, 0].set_xlabel('Flare Class', fontsize=12)
    axes[1, 0].set_ylabel('Radio Blackout Probability', fontsize=12)
    axes[1, 0].set_title('Blackout Probability by Flare Class', fontsize=14, fontweight='bold')
    plt.suptitle('')

    # 4. Box plot by blackout severity
    matched_df.boxplot(column='solar_flare_probability', by='blackout_severity', ax=axes[1, 1])
    axes[1, 1].set_xlabel('Blackout Severity', fontsize=12)
    axes[1, 1].set_ylabel('Solar Flare Probability', fontsize=12)
    axes[1, 1].set_title('Flare Probability by Blackout Severity', fontsize=14, fontweight='bold')
    plt.suptitle('')

    plt.tight_layout()
    plt.show()
else:
    print("Cannot create visualizations - no matched events found.")

"""### 7.3 Statistical Pattern Analysis"""

if len(matched_df) > 0:
    print("PATTERN ANALYSIS BY CATEGORIES")
    print("="*80)

    # Analyze by flare class
    print("\n1. Average Radio Blackout Probability by Flare Class:")
    print("-" * 60)
    flare_analysis = matched_df.groupby('flare_class')['radio_blackout_probability'].agg([
        'count', 'mean', 'std', 'min', 'max'
    ]).round(4)
    print(flare_analysis)

    # Analyze by blackout severity
    print("\n2. Average Solar Flare Probability by Blackout Severity:")
    print("-" * 60)
    severity_analysis = matched_df.groupby('blackout_severity')['solar_flare_probability'].agg([
        'count', 'mean', 'std', 'min', 'max'
    ]).round(4)
    print(severity_analysis)

    # Cross-tabulation
    print("\n3. Cross-tabulation: Flare Class vs Blackout Severity")
    print("-" * 60)
    cross_tab = pd.crosstab(matched_df['flare_class'], matched_df['blackout_severity'])
    print(cross_tab)
else:
    print("Cannot perform pattern analysis - no matched events found.")

"""### 7.4 Binned Analysis

Let's group probabilities into bins to see clearer patterns:
"""

if len(matched_df) > 0:
    # Create probability bins
    matched_df['solar_flare_bin'] = pd.cut(
        matched_df['solar_flare_probability'],
        bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],
        labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']
    )

    matched_df['radio_blackout_bin'] = pd.cut(
        matched_df['radio_blackout_probability'],
        bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],
        labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']
    )

    print("BINNED PROBABILITY ANALYSIS")
    print("="*80)

    # Create contingency table
    contingency = pd.crosstab(
        matched_df['solar_flare_bin'],
        matched_df['radio_blackout_bin'],
        margins=True
    )

    print("\nContingency Table: Solar Flare Bin vs Radio Blackout Bin")
    print("-" * 80)
    print(contingency)

    # Visualize the pattern
    plt.figure(figsize=(10, 8))
    sns.heatmap(contingency.iloc[:-1, :-1], annot=True, fmt='d', cmap='YlOrRd',
                cbar_kws={'label': 'Count'})
    plt.title('Pattern Matrix: Solar Flare vs Radio Blackout Probability Bins',
              fontsize=14, fontweight='bold')
    plt.xlabel('Radio Blackout Probability Bin', fontsize=12)
    plt.ylabel('Solar Flare Probability Bin', fontsize=12)
    plt.tight_layout()
    plt.show()
else:
    print("Cannot perform binned analysis - no matched events found.")

"""---
## Step 8: Time-Based Pattern Analysis
"""

if len(matched_df) > 0:
    # Analyze time difference patterns
    print("TIME DIFFERENCE ANALYSIS")
    print("="*80)
    print("\nStatistics of time difference between matched events:")
    print(matched_df['time_difference_hours'].describe())

    # Visualize time difference
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # Histogram of time differences
    axes[0].hist(matched_df['time_difference_hours'], bins=30,
                 color='mediumpurple', edgecolor='black')
    axes[0].set_xlabel('Time Difference (hours)', fontsize=12)
    axes[0].set_ylabel('Frequency', fontsize=12)
    axes[0].set_title('Distribution of Time Differences', fontsize=14, fontweight='bold')
    axes[0].grid(alpha=0.3)

    # Scatter: time difference vs correlation strength
    axes[1].scatter(matched_df['time_difference_hours'],
                    abs(matched_df['solar_flare_probability'] - matched_df['radio_blackout_probability']),
                    alpha=0.5, c='green', edgecolors='black')
    axes[1].set_xlabel('Time Difference (hours)', fontsize=12)
    axes[1].set_ylabel('Probability Difference (absolute)', fontsize=12)
    axes[1].set_title('Time Lag vs Probability Difference', fontsize=14, fontweight='bold')
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.show()
else:
    print("Cannot perform time-based analysis - no matched events found.")

"""---
## Step 9: Summary and Key Findings
"""

print("="*80)
print("PROJECT SUMMARY: SOLAR FLARE & RADIO BLACKOUT PATTERN ANALYSIS")
print("="*80)

print("\n1. DATA PROCESSING:")
print("-" * 60)
print(f"   • Solar Flare Events: {len(solar_df):,}")
print(f"   • Radio Blackout Events: {len(radio_blackout_df):,}")
print(f"   • Matched Events (within 24h window): {len(matched_df):,}")

print("\n2. PROBABILITY GENERATION:")
print("-" * 60)
print(f"   • Solar Flare Probabilities:")
print(f"     - Range: [{solar_df['solar_flare_probability'].min():.4f}, {solar_df['solar_flare_probability'].max():.4f}]")
print(f"     - Mean: {solar_df['solar_flare_probability'].mean():.4f}")
print(f"     - Unique values: {solar_df['solar_flare_probability'].nunique():,}")

print(f"\n   • Radio Blackout Probabilities:")
print(f"     - Range: [{radio_blackout_df['radio_blackout_probability'].min():.4f}, {radio_blackout_df['radio_blackout_probability'].max():.4f}]")
print(f"     - Mean: {radio_blackout_df['radio_blackout_probability'].mean():.4f}")
print(f"     - Unique values: {radio_blackout_df['radio_blackout_probability'].nunique():,}")

if len(matched_df) > 0:
    print("\n3. PATTERN FINDINGS:")
    print("-" * 60)
    print(f"   • Pearson Correlation: {pearson_corr:.4f} (p={pearson_p:.6f})")
    print(f"   • Spearman Correlation: {spearman_corr:.4f} (p={spearman_p:.6f})")

    if abs(pearson_corr) > 0.4:
        print("\n   ✓ There IS a notable relationship between solar flare probability")
        print("     and radio blackout probability in the matched events.")
    else:
        print("\n   ⚠ The relationship between solar flare probability and radio")
        print("     blackout probability is weak or complex in nature.")

    print("\n4. RECOMMENDATIONS:")
    print("-" * 60)
    print("   • Review the visualizations above for pattern insights")
    print("   • Consider different time windows for matching (currently 24h)")
    print("   • Explore non-linear relationships with advanced models")
    print("   • Consider lag effects (blackouts may occur hours after flares)")
else:
    print("\n3. PATTERN FINDINGS:")
    print("-" * 60)
    print("   ⚠ No matched events found within the 24-hour time window.")
    print("   • This could mean:")
    print("     - Datasets cover different time periods")
    print("     - Time window needs adjustment")
    print("     - More data is needed for analysis")

print("\n" + "="*80)
print("ANALYSIS COMPLETE!")
print("="*80)

"""---
## Step 10: Export Results

Let's save our processed datasets with probabilities:
"""

# Export solar flare data with probabilities
solar_export = solar_df[['timestamp', 'flare_class', 'solar_flare_probability',
                         'xray_flux_short', 'xray_flux_long', 'magnetic_field',
                         'sunspot_number', 'solar_wind_speed', 'proton_flux']].copy()
solar_export.to_csv('/content/drive/MyDrive/new/output2/solar_flare_with_probabilities.csv', index=False)
print("✓ Saved: solar_flare_with_probabilities.csv")

# Export radio blackout data with probabilities
blackout_export = radio_blackout_df[['start_time', 'end_time', 'severity',
                                      'duration_hours', 'radio_blackout_probability']].copy()
blackout_export.to_csv('/content/drive/MyDrive/new/output2/radio_blackout_with_probabilities.csv', index=False)
print("✓ Saved: radio_blackout_with_probabilities.csv")

# Export matched events
if len(matched_df) > 0:
    matched_df.to_csv('/content/drive/MyDrive/new/output2/matched_events_analysis.csv', index=False)
    print("✓ Saved: matched_events_analysis.csv")

print("\n✓ All results exported successfully!")

"""---
## Additional Notes for Beginners

### What You Learned:

1. **Data Loading**: How to read CSV files and explore datasets
2. **Data Preprocessing**: Handling missing values, encoding categories
3. **Feature Engineering**: Creating new meaningful columns from existing data
4. **Normalization**: Scaling features to comparable ranges (0-1)
5. **Probability Calculation**: Combining multiple features with weights
6. **Data Alignment**: Matching events across different datasets
7. **Correlation Analysis**: Measuring relationships between variables
8. **Data Visualization**: Creating meaningful plots and charts
9. **Pattern Recognition**: Identifying trends and relationships

### Key Concepts:

- **Probability**: A measure (0 to 1) of how likely an event is
- **Correlation**: How two variables move together (-1 to +1)
- **Feature**: A measurable property used for analysis
- **Normalization**: Scaling data to a standard range
- **Time Window**: Period for matching events

### Next Steps:

1. Experiment with different time windows
2. Try different weight combinations
3. Explore machine learning models for prediction
4. Add more features or external data
5. Investigate lag effects (e.g., do blackouts occur X hours after flares?)

---

**Remember**: This analysis identifies *patterns* in historical data, not predictions for the future!
"""