# Finetuning tasks

This repository contains downstream finetuning code related to six key tasks that are structured as prediction or segmentation. These tasks are related to various aspects of solar and heliospheric events ecosystem and are listed below:
1. **[Active Region Segmentation](ar_segmentation/)**
2. **[Solar EUV Spectra Modeling](euv_spectra_prediction/)**
3. **[Solar Flare Forecasting](solar_flare_forcasting/)**
4. **[Solar Wind Forecasting](solar_wind_forcasting/)**

### Inference tasks

For inference tasks, we have provided a end to end notebook to run the inference pipeline which includes downloading sample SDO data, model weights and running the inference pipeline.

1. **[ðŸ“’ Active Region Segmentation Notebook](ar_segmentation/ar_segmentation_tutorial.ipynb)**
2. **[ðŸ“’ Solar EUV Spectra Modeling Notebook](euv_spectra_prediction/euv_spectra_tutorial.ipynb)**
3. **[ðŸ“’ Solar Flare Forecasting Notebook](solar_flare_forcasting/solar_flare_tutorial.ipynb)**
4. **[ðŸ“’ Solar Wind Forecasting Notebook](solar_wind_forcasting/solar_wind_tutorial.ipynb)**


## (Optional) Download SDO Data for Training

The data is located at [nasa-ibm-ai4science/core-sdo](https://huggingface.co/datasets/nasa-ibm-ai4science/core-sdo)

Downloads and generates csv files. The `data_path` is defined in the main function

1. Downloads the data from huggingface repo -> `data_path/tars/{validate,test}_splitted_tars/`
2. combines the extracted tar parts -> `data_path/tars/{validate,test}_splitted_tars/{validate,test}.tar`
3. Extracts data from the tar file -> `data_path/{validate,test}/*.nc`
4. CSV files (copies the same csv to downstream_tasks) -> `{downstream_tasks}/assets/sdo_{validate, test}.csv`


### Run the python file to download data

```bash
python download_sample_train_data.py
```

The structure of the downloaded file should give the following structure:

```
downstream_examples
â”œâ”€â”€ ar_segmentation
â”‚   â”œâ”€â”€ assets
â”‚   â”‚   â”œâ”€â”€ sdo_test.csv
â”‚   â”‚   â””â”€â”€ sdo_validate.csv
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ common_data
â”‚   â”œâ”€â”€ tars
â”‚   â”‚   â”œâ”€â”€ test_splitted_tars
â”‚   â”‚   â”‚   â”œâ”€â”€ test.tar
â”‚   â”‚   â”‚   â”œâ”€â”€ test.tar.part_aa
â”‚   â”‚   â”‚   â”œâ”€â”€ test.tar.part_aa
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”‚   â””â”€â”€ test.tar.part_aq
â”‚   â”‚   â””â”€â”€ validate_splitted_tars
â”‚   â”‚       â”œâ”€â”€ validate.tar
â”‚   â”‚       â”œâ”€â”€ validate.tar.part_aa
â”‚   â”‚       â”œâ”€â”€ validate.tar.part_aa
â”‚   â”‚       â”œâ”€â”€ ...
â”‚   â”‚       â”œâ”€â”€ ...
â”‚   â”‚       â””â”€â”€ validate.tar.part_aq
â”‚   â”œâ”€â”€ test
â”‚   â”‚   â”œâ”€â”€ 20110103_1200.nc
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ 20110105_2348.nc
â”‚   â””â”€â”€ validate
â”‚       â”œâ”€â”€ 20110106_0000.nc
â”‚       â”œâ”€â”€ ...
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ 20110108_1148.nc
â”œâ”€â”€ download_data.py
â”œâ”€â”€ euv_spectra_prediction
â”‚   â”œâ”€â”€ assets
â”‚   â”‚   â”œâ”€â”€ sdo_test.csv
â”‚   â”‚   â””â”€â”€ sdo_validate.csv
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ README.md
â”œâ”€â”€ solar_flare_forcasting
â”‚   â”œâ”€â”€ assets
â”‚   â”‚   â”œâ”€â”€ sdo_test.csv
â”‚   â”‚   â””â”€â”€ sdo_validate.csv
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ ...
â””â”€â”€ solar_wind_forcasting
    â”œâ”€â”€ assets
    â”‚   â”œâ”€â”€ sdo_test.csv
    â”‚   â””â”€â”€ sdo_validate.csv
    â”œâ”€â”€ ...
    â””â”€â”€ ...
```

The created `sdo_test.csv` and `sdo_validate.csv` are created with the following format

```
,path,timestep,present
0,/full/path/to/downstream_examples/common_data/test/20110101_0000.nc,2011-01-01 00:00:00,0
1,/full/path/to/downstream_examples/common_data/test/20110101_0012.nc,2011-01-01 00:12:00,0
2,/full/path/to/downstream_examples/common_data/test/20110101_0024.nc,2011-01-01 00:24:00,1
3,/full/path/to/downstream_examples/common_data/test/20110101_0036.nc,2011-01-01 00:36:00,1
```

- `path`: path to the nc files
- `timestamp`: timetamp of the data
- `present`: 1 represent we have the nc file.


Note: 
1. In the `sdo_{test,validate}.csv`, an entry for every 12 minute cadence are created from the start date and month to end date and month. The `present` column represent if we have the nc file. So, if we have 297 files and two month of range we will have 6962 entries, with 297 entries having `1` and the rest with `0`. These are used by the dataloaders to determine the files to use.
2. These csv are created for all the downstreams present in `downstream_tasks` variable in the `download_sample_train_data.py` file.
3. All the tar part files must be downloaded to create a complete tar file. If any of the data is missing, we cannot create a tar file and extract the `nc` files. So be sure to have enough data space for the 3 steps:
    a. part tars
    b. complete (combined) tars
    c. Extracted nc files
4. The intermediate files are not deleted automatically. This is left for the user to cleanup.
