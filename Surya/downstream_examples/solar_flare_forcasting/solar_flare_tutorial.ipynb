{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Solar Flare Forecasting Tutorial for Beginners ðŸŒž\n",
    "\n",
    "This notebook will guide you through running Solar Flare forecasting using the Surya model. \n",
    "\n",
    "## What you'll learn:\n",
    "- How to load a pre-trained model for solar flare prediction\n",
    "- How to run inference on solar data\n",
    "- How to interpret forecasting results\n",
    "\n",
    "## Prerequisites:\n",
    "- Make sure you're in the correct directory: `downstream_examples/solar_flare_forcasting/`\n",
    "- Ensure all required packages are installed (torch, yaml, matplotlib, numpy, etc.)\n",
    "\n",
    "Let's get started! ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackupnfs1/sroy14/rohit/surya_worktree/main/.venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful!\n",
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Import functions from our inference script\n",
    "from infer import (\n",
    "    load_model, \n",
    "    run_inference,\n",
    "    get_dataloader,\n",
    "    infer_single_sample\n",
    ")\n",
    "\n",
    "# Import from surya\n",
    "from surya.utils.data import build_scalers\n",
    "from surya.utils.distributed import set_global_seed\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Pre-trained Model Weights\n",
    "The model weights will be downloaded automatically from Hugging Face.\n",
    "This may take a few minutes the first time you run the code.\n",
    "\n",
    "For downloading the dataset (if not already present locally):\n",
    "- If the cell below fails, try running the provided shell script directly in the terminal.\n",
    "- Sometimes the download may fail due to network or server issuesâ€”if that happens, simply re-run the script a few times until it completes successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Checking assets directory at: /nobackupnfs1/sroy14/rohit/surya_worktree/main/downstream_examples/solar_flare_forcasting/assets\n",
      "==> Downloading dataset 'nasa-ibm-ai4science/surya-bench-flare-forecasting' to '/nobackupnfs1/sroy14/rohit/surya_worktree/main/downstream_examples/solar_flare_forcasting/assets/surya-bench-flare-forecasting'\n",
      "Fetching 6 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 86.37it/s]\n",
      "/nobackupnfs1/sroy14/rohit/surya_worktree/main/downstream_examples/solar_flare_forcasting/assets/surya-bench-flare-forecasting\n",
      "Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 80.61it/s]\n",
      "/nobackupnfs1/sroy14/rohit/surya_worktree/main/downstream_examples/solar_flare_forcasting/assets\n",
      "Fetching 19 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 258.36it/s]\n",
      "/nobackupnfs1/sroy14/rohit/surya_worktree/main/downstream_examples/solar_flare_forcasting/assets\n",
      "âœ“ Done. Files are in: /nobackupnfs1/sroy14/rohit/surya_worktree/main/downstream_examples/solar_flare_forcasting/assets/surya-bench-flare-forecasting\n"
     ]
    }
   ],
   "source": [
    "!sh download_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set Up Configuration\n",
    "\n",
    "We need to load the configuration file that contains all the model and data parameters. Make sure you have a `config.yaml` file in your current directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Loading configuration...\n",
      "âœ… Configuration loaded successfully!\n",
      "Model type: spectformer\n",
      "Data precision: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Configuration paths - modify these if your files are in different locations\n",
    "config_path = \"./config_infer.yaml\"\n",
    "checkpoint_path = \"./assets/solar_flare_weights.pth\"\n",
    "output_dir = \"./inference_results\"\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "set_global_seed(42)\n",
    "\n",
    "# Load configuration\n",
    "print(\"ðŸ“‹ Loading configuration...\")\n",
    "try:\n",
    "    config = yaml.safe_load(open(config_path, \"r\"))\n",
    "    config[\"data\"][\"scalers\"] = yaml.safe_load(open(config[\"data\"][\"scalers_path\"], \"r\"))\n",
    "    print(\"âœ… Configuration loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"Make sure config.yaml exists in your current directory\")\n",
    "    raise\n",
    "\n",
    "# Set data type (float precision)\n",
    "if config[\"dtype\"] == \"float16\":\n",
    "    config[\"dtype\"] = torch.float16\n",
    "elif config[\"dtype\"] == \"bfloat16\":\n",
    "    config[\"dtype\"] = torch.bfloat16\n",
    "elif config[\"dtype\"] == \"float32\":\n",
    "    config[\"dtype\"] = torch.float32\n",
    "else:\n",
    "    raise NotImplementedError(\"Please choose from [float16,bfloat16,float32]\")\n",
    "\n",
    "print(f\"Model type: {config['model']['model_type']}\")\n",
    "print(f\"Data precision: {config['dtype']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Device (GPU/CPU)\n",
    "\n",
    "Let's determine whether to use GPU or CPU for inference. GPU is much faster if available!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ Using CPU (this will be slower)\n",
      "ðŸ’¡ Tip: Consider using a machine with GPU for faster inference\n"
     ]
    }
   ],
   "source": [
    "# Set device - automatically use GPU if available, otherwise CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"ðŸš€ Using GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"ðŸŒ Using CPU (this will be slower)\")\n",
    "    print(\"ðŸ’¡ Tip: Consider using a machine with GPU for faster inference\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Run Solar Flare Forecasting (Easy Method)\n",
    "\n",
    "This is the simplest way to run inference. The `run_inference` function handles everything for you and will show predictions vs ground truth!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Starting solar flare forecasting inference...\n",
      "Loading model from ./assets/solar_flare_weights.pth\n",
      "Loading pretrained model from ../../data/Surya-1.0/surya.366m.v1.pt.\n",
      "Applying PEFT LoRA with configuration: {'r': 8, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj', 'k_proj', 'out_proj', 'fc1', 'fc2'], 'lora_dropout': 0.1, 'bias': 'none'}\n",
      "trainable params: 1,024,000 || all params: 364,593,153 || trainable%: 0.28%\n",
      "Dataset size: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackupnfs1/sroy14/rohit/surya_worktree/main/.venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/nobackupnfs1/sroy14/rohit/surya_worktree/main/.venv/lib64/python3.11/site-packages/torch/amp/autocast_mode.py:283: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Time Input           | Time Target          | Prediction      | Ground Truth\n",
      "--------------------------------------------------------------------------------\n",
      "2011-01-20T01:00     | 2011-01-20T03:00     | 0               | 0           \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Time Input           | Time Target          | Prediction      | Ground Truth\n",
      "--------------------------------------------------------------------------------\n",
      "2019-01-23T02:00     | 2019-01-23T04:00     | 0               | 0           \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Time Input           | Time Target          | Prediction      | Ground Truth\n",
      "--------------------------------------------------------------------------------\n",
      "2012-01-15T01:00     | 2012-01-15T03:00     | 1               | 0           \n",
      "================================================================================\n",
      "ðŸŽ‰ Solar flare forecasting completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Parameters for inference\n",
    "data_type = \"test\"  # or \"valid\" - which dataset to use\n",
    "num_samples = 3  # Number of samples to process and analyze\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"ðŸ”¬ Starting solar flare forecasting inference...\")\n",
    "# Run the complete inference pipeline\n",
    "try:\n",
    "    run_inference(\n",
    "        config=config,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        output_dir=output_dir,\n",
    "        device=device,\n",
    "        data_type=data_type,\n",
    "        num_samples=num_samples,\n",
    "        device_type=device_type\n",
    "    )\n",
    "    print(\"ðŸŽ‰ Solar flare forecasting completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during inference: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Understanding the Results ðŸ“Š\n",
    "\n",
    "### What you're seeing:\n",
    "- **Time Input**: The timestamp of the input solar observations\n",
    "- **Time Target**: The timestamp for which we're making the flare prediction\n",
    "- **Prediction**: The model's binary prediction (0 = No Flare, 1 = Flare)\n",
    "- **Ground Truth**: The actual observed outcome (0 = No Flare, 1 = Flare)\n",
    "\n",
    "### Tips for interpretation:\n",
    "1. **Prediction = Ground Truth**: The model made a correct prediction\n",
    "2. **Prediction â‰  Ground Truth**: The model made an incorrect prediction\n",
    "3. **Time difference**: Shows how far ahead the model is forecasting\n",
    "4. **Multiple samples**: Each row shows a different solar observation and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Troubleshooting ðŸ”§\n",
    "\n",
    "### Common issues:\n",
    "1. **\"No config.yaml found\"**: Make sure you have the configuration file in your directory\n",
    "2. **\"No data found\"**: Check that your data paths in config.yaml are correct\n",
    "3. **Import errors**: Ensure all required packages are installed\n",
    "4. **CUDA errors**: Make sure your GPU has enough memory or switch to CPU\n",
    "\n",
    "### Need help?\n",
    "- Check the original `infer.py` file for more details\n",
    "- Verify your data paths in the configuration\n",
    "- Make sure you're in the correct directory: `downstream_examples/solar_flare_forcasting/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary ðŸŽ¯\n",
    "\n",
    "Congratulations! You've successfully run solar flare forecasting using the Surya model. \n",
    "\n",
    "### What you accomplished:\n",
    "âœ… Downloaded pre-trained model weights  \n",
    "âœ… Loaded and configured the model  \n",
    "âœ… Ran inference on solar data  \n",
    "âœ… Generated flare predictions with timestamps  \n",
    "âœ… Compared predictions with ground truth  \n",
    "âœ… Learned about manual inference options  \n",
    "\n",
    "### Next steps:\n",
    "- Try different numbers of samples\n",
    "- Experiment with different data types (test vs valid)\n",
    "- Analyze the prediction accuracy patterns\n",
    "- Check out the original `infer.py` for more advanced usage\n",
    "\n",
    "### Understanding Solar Flare Forecasting:\n",
    "- **Binary Classification**: The model predicts whether a solar flare will occur (1) or not (0)\n",
    "- **Time-based Prediction**: Uses current solar observations to predict future flare activity\n",
    "- **Practical Applications**: Helps protect satellites, power grids, and astronauts from space weather\n",
    "\n",
    "Happy solar flare forecasting! ðŸŒžâš¡âœ¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
