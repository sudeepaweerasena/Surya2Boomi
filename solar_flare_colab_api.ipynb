{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# üåû Solar Flare Forecasting API - Google Colab\n",
                "\n",
                "This notebook sets up a complete solar flare forecasting system using NASA IMPACT's pretrained Surya model.\n",
                "\n",
                "**Features:**\n",
                "- üöÄ Automated data and model download\n",
                "- üîÆ 24-hour solar flare probability forecasting\n",
                "- üåê REST API with ngrok tunnel for remote access\n",
                "- üîê API key authentication\n",
                "\n",
                "**Usage:**\n",
                "1. Run all cells sequentially\n",
                "2. Copy the ngrok URL when displayed\n",
                "3. Use the local client script to make API calls\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup-header"
            },
            "source": [
                "## üì¶ Step 1: Environment Setup & Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install-dependencies"
            },
            "outputs": [],
            "source": [
                "# Suppress warnings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore', category=FutureWarning)\n",
                "\n",
                "# Install required packages\n",
                "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                "!pip install -q transformers huggingface-hub einops timm\n",
                "!pip install -q flask flask-cors pyngrok\n",
                "!pip install -q netCDF4 xarray h5py scipy numpy pandas matplotlib\n",
                "!pip install -q PyYAML tqdm peft\n",
                "\n",
                "print(\"‚úÖ All dependencies installed successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "check-gpu"
            },
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "import torch\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  Warning: No GPU detected. Inference will be slower on CPU.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "clone-header"
            },
            "source": [
                "## üîÑ Step 2: Clone Repository & Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone-repo"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Clone the Surya repository\n",
                "if not os.path.exists('/content/Surya'):\n",
                "    !git clone https://github.com/NASA-IMPACT/Surya.git\n",
                "    print(\"‚úÖ Repository cloned successfully\")\n",
                "else:\n",
                "    print(\"‚ÑπÔ∏è  Repository already exists\")\n",
                "\n",
                "# Change to the solar flare forecasting directory\n",
                "os.chdir('/content/Surya/downstream_examples/solar_flare_forcasting')\n",
                "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
                "\n",
                "# Add to Python path\n",
                "sys.path.insert(0, '/content/Surya/downstream_examples/solar_flare_forcasting')\n",
                "sys.path.insert(0, '/content/Surya')\n",
                "\n",
                "print(\"‚úÖ Python path configured\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "download-header"
            },
            "source": [
                "## üíæ Step 3: Download Data & Pretrained Models\n",
                "\n",
                "This will download:\n",
                "- Surya foundation model weights\n",
                "- Solar flare task-specific weights\n",
                "- SDO data and scaling factors\n",
                "- Benchmark dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "huggingface-login"
            },
            "outputs": [],
            "source": [
                "# Login to Hugging Face (optional but recommended)\n",
                "from huggingface_hub import login\n",
                "\n",
                "# If you have a Hugging Face token, enter it here\n",
                "# login(token=\"your_token_here\")\n",
                "\n",
                "# OR run this cell and follow the prompt:\n",
                "# login()\n",
                "\n",
                "print(\"‚ÑπÔ∏è  You can skip HF login for public models, or login for better rate limits\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download-data"
            },
            "outputs": [],
            "source": [
                "# Execute the download script\n",
                "import subprocess\n",
                "\n",
                "print(\"üì• Starting download... This may take 5-10 minutes\")\n",
                "print(\"\" + \"=\"*60)\n",
                "\n",
                "# Run the download script\n",
                "result = subprocess.run(['bash', 'download_data.sh'], capture_output=True, text=True)\n",
                "\n",
                "if result.returncode == 0:\n",
                "    print(\"‚úÖ Data and models downloaded successfully!\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  Download completed with warnings (this is usually OK)\")\n",
                "    \n",
                "print(\"\\nüìä Downloaded assets:\")\n",
                "!ls -lh assets/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "model-header"
            },
            "source": [
                "## ü§ñ Step 4: Load Pretrained Model & Setup Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load-model"
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import yaml\n",
                "import numpy as np\n",
                "import torch.nn.functional as F\n",
                "from pathlib import Path\n",
                "\n",
                "# Import necessary modules from the Surya repository\n",
                "from surya.utils.data import build_scalers\n",
                "from surya.utils.distributed import set_global_seed\n",
                "from dataset import SolarFlareDataset\n",
                "from finetune import get_model, apply_peft_lora, custom_collate_fn\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "set_global_seed(42)\n",
                "\n",
                "# Load configuration\n",
                "with open('config_infer.yaml', 'r') as f:\n",
                "    config = yaml.safe_load(f)\n",
                "\n",
                "# Load scalers\n",
                "config[\"data\"][\"scalers\"] = yaml.safe_load(open(config[\"data\"][\"scalers_path\"], \"r\"))\n",
                "scalers = build_scalers(info=config[\"data\"][\"scalers\"])\n",
                "\n",
                "# Set dtype\n",
                "if config[\"dtype\"] == \"float16\":\n",
                "    config[\"dtype\"] = torch.float16\n",
                "elif config[\"dtype\"] == \"bfloat16\":\n",
                "    config[\"dtype\"] = torch.bfloat16\n",
                "elif config[\"dtype\"] == \"float32\":\n",
                "    config[\"dtype\"] = torch.float32\n",
                "else:\n",
                "    raise NotImplementedError(\"Please choose from [float16,bfloat16,float32]\")\n",
                "\n",
                "print(\"üìã Configuration loaded\")\n",
                "print(f\"   Model type: {config['model']['model_type']}\")\n",
                "print(f\"   Task: Solar Flare Forecasting (Binary Classification)\")\n",
                "print(f\"   Using LoRA: {config['model']['use_lora']}\")\n",
                "\n",
                "# Initialize model using the repository's helper function\n",
                "print(\"\\nüîß Initializing model...\")\n",
                "model = get_model(config, wandb_logger=None)\n",
                "\n",
                "# Apply LoRA if configured\n",
                "if config[\"model\"][\"use_lora\"]:\n",
                "    print(\"   Applying PEFT LoRA...\")\n",
                "    model = apply_peft_lora(model, config)\n",
                "\n",
                "# Load checkpoint weights\n",
                "checkpoint_path = './assets/solar_flare_weights.pth'\n",
                "if os.path.exists(checkpoint_path):\n",
                "    print(f\"\\nüì• Loading checkpoint from {checkpoint_path}...\")\n",
                "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
                "    \n",
                "    # Handle different checkpoint formats\n",
                "    if 'model_state_dict' in checkpoint:\n",
                "        model_state = checkpoint['model_state_dict']\n",
                "    elif 'state_dict' in checkpoint:\n",
                "        model_state = checkpoint['state_dict']\n",
                "    else:\n",
                "        model_state = checkpoint\n",
                "    \n",
                "    # Remove 'module.' prefix if present (from DistributedDataParallel)\n",
                "    if any(key.startswith('module.') for key in model_state.keys()):\n",
                "        model_state = {key.replace('module.', ''): value for key, value in model_state.items()}\n",
                "    \n",
                "    # Load state dict\n",
                "    try:\n",
                "        model.load_state_dict(model_state, strict=True)\n",
                "        print(\"‚úÖ Loaded pretrained weights successfully!\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è  Failed to load with strict=True: {e}\")\n",
                "        print(\"   Trying with strict=False...\")\n",
                "        model.load_state_dict(model_state, strict=False)\n",
                "        print(\"‚úÖ Loaded weights (some keys ignored)\")\n",
                "else:\n",
                "    print(f\"‚ö†Ô∏è  Warning: Checkpoint not found at {checkpoint_path}\")\n",
                "    print(\"   Model will use only foundation weights\")\n",
                "\n",
                "# Move model to device and set to evaluation mode\n",
                "model = model.to(device)\n",
                "model.eval()\n",
                "\n",
                "print(f\"\\n‚úÖ Model loaded and ready for inference!\")\n",
                "print(f\"   Device: {device}\")\n",
                "print(f\"   Data type: {config['dtype']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "inference-function"
            },
            "outputs": [],
            "source": [
                "# Define 24-hour forecasting function\n",
                "import datetime\n",
                "import json\n",
                "from torch.utils.data import DataLoader, Subset\n",
                "\n",
                "def forecast_24_hours(model, config, scalers, device):\n",
                "    \"\"\"\n",
                "    Generate 24-hour solar flare probability forecast\n",
                "    \n",
                "    Returns:\n",
                "        dict: Forecast results with timestamps and probabilities\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Create dataset\n",
                "        dataset = SolarFlareDataset(\n",
                "            sdo_data_root_path=config[\"data\"][\"sdo_data_root_path\"],\n",
                "            index_path=config[\"data\"][\"valid_data_path\"],\n",
                "            time_delta_input_minutes=config[\"data\"][\"time_delta_input_minutes\"],\n",
                "            time_delta_target_minutes=config[\"data\"][\"time_delta_target_minutes\"],\n",
                "            n_input_timestamps=config[\"model\"][\"time_embedding\"][\"time_dim\"],\n",
                "            rollout_steps=config[\"rollout_steps\"],\n",
                "            channels=config[\"data\"][\"channels\"],\n",
                "            drop_hmi_probability=config[\"drop_hmi_probability\"],\n",
                "            num_mask_aia_channels=config[\"num_mask_aia_channels\"],\n",
                "            use_latitude_in_learned_flow=config[\"use_latitude_in_learned_flow\"],\n",
                "            scalers=scalers,\n",
                "            phase=\"valid\",\n",
                "            flare_index_path=config[\"data\"][\"flare_data_path\"],\n",
                "            pooling=config[\"data\"][\"pooling\"],\n",
                "            random_vert_flip=False,\n",
                "        )\n",
                "        \n",
                "        if len(dataset) == 0:\n",
                "            return {\n",
                "                'status': 'error',\n",
                "                'message': 'No data available for forecasting'\n",
                "            }\n",
                "        \n",
                "        # Get samples for forecasting (up to 24 samples or all available)\n",
                "        num_samples = min(24, len(dataset))\n",
                "        \n",
                "        # Use the last samples as they are most recent\n",
                "        sample_indices = list(range(max(0, len(dataset) - num_samples), len(dataset)))\n",
                "        \n",
                "        dataloader = DataLoader(\n",
                "            dataset=Subset(dataset, sample_indices),\n",
                "            batch_size=1,\n",
                "            num_workers=0,  # Use 0 for simplicity in Colab\n",
                "            pin_memory=True,\n",
                "            shuffle=False,\n",
                "            collate_fn=custom_collate_fn,\n",
                "        )\n",
                "        \n",
                "        # Run inference\n",
                "        base_time = datetime.datetime.now(datetime.timezone.utc)\n",
                "        forecast_data = []\n",
                "        \n",
                "        model.eval()\n",
                "        with torch.no_grad():\n",
                "            for hour, (batch, metadata) in enumerate(dataloader):\n",
                "                if hour >= 24:  # Limit to 24 hours\n",
                "                    break\n",
                "                    \n",
                "                # Move batch to device\n",
                "                batch = {k: v.to(device) for k, v in batch.items()}\n",
                "                \n",
                "                # Get ground truth\n",
                "                ground_truth = batch[\"label\"].item()\n",
                "                timestamps_input = metadata[\"timestamps_input\"]\n",
                "                timestamps_targets = metadata[\"timestamps_targets\"]\n",
                "                \n",
                "                # Run inference with mixed precision\n",
                "                device_type = \"cuda\" if device.type == \"cuda\" else \"cpu\"\n",
                "                with torch.amp.autocast(device_type=device_type, dtype=config[\"dtype\"]):\n",
                "                    logits = model(batch)\n",
                "                    # Convert to probability (sigmoid for binary classification)\n",
                "                    flare_probability = float(F.sigmoid(logits).item())\n",
                "                \n",
                "                # Convert numpy datetime64 to readable string format\n",
                "                timestamps_target_str = np.datetime_as_string(timestamps_targets, unit='m')[0][0]\n",
                "                \n",
                "                # Calculate forecast time\n",
                "                forecast_time = base_time + datetime.timedelta(hours=hour)\n",
                "                \n",
                "                # Determine flare class based on probability\n",
                "                if flare_probability > 0.7:\n",
                "                    flare_class = 'M'\n",
                "                elif flare_probability > 0.4:\n",
                "                    flare_class = 'C'\n",
                "                else:\n",
                "                    flare_class = 'B'\n",
                "                \n",
                "                forecast_data.append({\n",
                "                    'hour': hour,\n",
                "                    'timestamp': forecast_time.isoformat(),\n",
                "                    'data_timestamp': timestamps_target_str,\n",
                "                    'no_flare_probability': float(1.0 - flare_probability),\n",
                "                    'flare_probability': flare_probability,\n",
                "                    'flare_class': flare_class,\n",
                "                    'ground_truth': int(ground_truth)\n",
                "                })\n",
                "        \n",
                "        result = {\n",
                "            'status': 'success',\n",
                "            'forecast_generated_at': base_time.isoformat(),\n",
                "            'forecast_horizon': f'{len(forecast_data)} hours',\n",
                "            'forecasts': forecast_data,\n",
                "            'model_info': {\n",
                "                'name': 'Surya Solar Flare Forecaster',\n",
                "                'model_type': config['model']['model_type'],\n",
                "                'version': '1.0',\n",
                "                'device': str(device),\n",
                "                'use_lora': config['model']['use_lora']\n",
                "            }\n",
                "        }\n",
                "        \n",
                "        return result\n",
                "            \n",
                "    except Exception as e:\n",
                "        import traceback\n",
                "        return {\n",
                "            'status': 'error',\n",
                "            'message': str(e),\n",
                "            'traceback': traceback.format_exc()\n",
                "        }\n",
                "\n",
                "# Test the forecasting function\n",
                "print(\"üß™ Testing forecast function...\")\n",
                "test_forecast = forecast_24_hours(model, config, scalers, device)\n",
                "\n",
                "if test_forecast['status'] == 'success':\n",
                "    print(\"‚úÖ Forecast function working!\")\n",
                "    print(f\"   Generated {len(test_forecast['forecasts'])} hourly predictions\")\n",
                "    print(f\"   First hour flare probability: {test_forecast['forecasts'][0]['flare_probability']:.4f}\")\n",
                "    print(f\"   Average flare probability: {np.mean([f['flare_probability'] for f in test_forecast['forecasts']]):.4f}\")\n",
                "else:\n",
                "    print(f\"‚ö†Ô∏è  Forecast test failed: {test_forecast['message']}\")\n",
                "    if 'traceback' in test_forecast:\n",
                "        print(f\"\\nTraceback:\\n{test_forecast['traceback']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "api-header"
            },
            "source": [
                "## üåê Step 5: Setup Flask API Server"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "flask-setup"
            },
            "outputs": [],
            "source": [
                "from flask import Flask, request, jsonify\n",
                "from flask_cors import CORS\n",
                "import secrets\n",
                "\n",
                "# Generate API key\n",
                "API_KEY = secrets.token_urlsafe(32)\n",
                "print(f\"üîë API Key: {API_KEY}\")\n",
                "print(\"‚ö†Ô∏è  Save this key! You'll need it for the local client.\\n\")\n",
                "\n",
                "# Initialize Flask app\n",
                "app = Flask(__name__)\n",
                "CORS(app)  # Enable CORS for all routes\n",
                "\n",
                "# Authentication middleware\n",
                "def require_api_key(f):\n",
                "    def decorated_function(*args, **kwargs):\n",
                "        api_key = request.headers.get('X-API-Key')\n",
                "        if api_key != API_KEY:\n",
                "            return jsonify({'error': 'Invalid or missing API key'}), 401\n",
                "        return f(*args, **kwargs)\n",
                "    decorated_function.__name__ = f.__name__\n",
                "    return decorated_function\n",
                "\n",
                "# Health check endpoint (no auth required)\n",
                "@app.route('/health', methods=['GET'])\n",
                "def health_check():\n",
                "    return jsonify({\n",
                "        'status': 'healthy',\n",
                "        'service': 'Solar Flare Forecasting API',\n",
                "        'version': '1.0',\n",
                "        'device': str(device)\n",
                "    }), 200\n",
                "\n",
                "# Status endpoint (with auth)\n",
                "@app.route('/status', methods=['GET'])\n",
                "@require_api_key\n",
                "def get_status():\n",
                "    return jsonify({\n",
                "        'model_loaded': True,\n",
                "        'device': str(device),\n",
                "        'data_available': os.path.exists('./assets'),\n",
                "        'ready_for_inference': True,\n",
                "        'model_type': config['model']['model_type'],\n",
                "        'use_lora': config['model']['use_lora']\n",
                "    }), 200\n",
                "\n",
                "# Forecast endpoint (with auth)\n",
                "@app.route('/forecast', methods=['POST'])\n",
                "@require_api_key\n",
                "def generate_forecast_endpoint():\n",
                "    try:\n",
                "        print(\"üìä Generating solar flare forecast...\")\n",
                "        result = forecast_24_hours(model, config, scalers, device)\n",
                "        \n",
                "        if result['status'] == 'success':\n",
                "            print(f\"‚úÖ Forecast generated: {len(result['forecasts'])} predictions\")\n",
                "            return jsonify(result), 200\n",
                "        else:\n",
                "            print(f\"‚ö†Ô∏è  Forecast generation failed: {result['message']}\")\n",
                "            return jsonify(result), 500\n",
                "            \n",
                "    except Exception as e:\n",
                "        import traceback\n",
                "        print(f\"‚ùå Error: {str(e)}\")\n",
                "        return jsonify({\n",
                "            'status': 'error',\n",
                "            'message': str(e),\n",
                "            'traceback': traceback.format_exc()\n",
                "        }), 500\n",
                "\n",
                "print(\"‚úÖ Flask API configured with 3 endpoints:\")\n",
                "print(\"   GET  /health   - Health check (no auth)\")\n",
                "print(\"   GET  /status   - System status (auth required)\")\n",
                "print(\"   POST /forecast - Generate forecast (auth required)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ngrok-header"
            },
            "source": [
                "## üåç Step 6: Start Ngrok Tunnel & API Server\n",
                "\n",
                "**Important:** After running this cell, copy the ngrok URL displayed below. You'll need it for the local client!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ngrok-setup"
            },
            "outputs": [],
            "source": [
                "from pyngrok import ngrok\n",
                "from threading import Thread\n",
                "\n",
                "# Set ngrok auth token (optional but recommended to avoid timeout)\n",
                "# Get free token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
                "# ngrok.set_auth_token(\"your_ngrok_token_here\")\n",
                "\n",
                "# Start ngrok tunnel\n",
                "public_url = ngrok.connect(5000)\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"üåç NGROK TUNNEL ACTIVE\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nüì° Public URL: {public_url}\")\n",
                "print(f\"üîë API Key: {API_KEY}\")\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"\\n‚úÖ Copy the URL and API Key above to use with the local client!\")\n",
                "print(\"\\n‚ö†Ô∏è  Keep this cell running to maintain the tunnel\\n\")\n",
                "\n",
                "# Start Flask server in background thread\n",
                "def run_flask():\n",
                "    app.run(host='0.0.0.0', port=5000, use_reloader=False)\n",
                "\n",
                "flask_thread = Thread(target=run_flask, daemon=True)\n",
                "flask_thread.start()\n",
                "\n",
                "print(\"üöÄ API Server is now running!\")\n",
                "print(\"\\nüìù Example API calls:\")\n",
                "print(f\"\\n   Health Check:\")\n",
                "print(f\"   curl {public_url}/health\")\n",
                "print(f\"\\n   Get Forecast:\")\n",
                "print(f\"   curl -X POST {public_url}/forecast \\\\\")\n",
                "print(f\"        -H 'X-API-Key: {API_KEY}'\")\n",
                "\n",
                "# Keep the cell running\n",
                "print(\"\\n‚è≥ Server running... Press 'Stop' button to terminate\")\n",
                "import time\n",
                "try:\n",
                "    while True:\n",
                "        time.sleep(1)\n",
                "except KeyboardInterrupt:\n",
                "    print(\"\\nüõë Server stopped\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "testing-header"
            },
            "source": [
                "## üß™ Step 7: Test API (Optional)\n",
                "\n",
                "You can test the API directly from Colab before using the local client."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test-api"
            },
            "outputs": [],
            "source": [
                "import requests\n",
                "\n",
                "# Test health endpoint\n",
                "print(\"üß™ Testing API endpoints...\\n\")\n",
                "\n",
                "ngrok_url = str(public_url)  # Use the URL from previous cell\n",
                "\n",
                "# Test 1: Health check\n",
                "print(\"1. Health Check:\")\n",
                "response = requests.get(f\"{ngrok_url}/health\")\n",
                "print(f\"   Status: {response.status_code}\")\n",
                "print(f\"   Response: {response.json()}\\n\")\n",
                "\n",
                "# Test 2: Status (with auth)\n",
                "print(\"2. Status Check:\")\n",
                "headers = {'X-API-Key': API_KEY}\n",
                "response = requests.get(f\"{ngrok_url}/status\", headers=headers)\n",
                "print(f\"   Status: {response.status_code}\")\n",
                "print(f\"   Response: {response.json()}\\n\")\n",
                "\n",
                "# Test 3: Generate forecast\n",
                "print(\"3. Generate Forecast:\")\n",
                "response = requests.post(f\"{ngrok_url}/forecast\", headers=headers)\n",
                "print(f\"   Status: {response.status_code}\")\n",
                "\n",
                "if response.status_code == 200:\n",
                "    forecast = response.json()\n",
                "    print(f\"   ‚úÖ Forecast generated successfully!\")\n",
                "    print(f\"   Generated at: {forecast['forecast_generated_at']}\")\n",
                "    print(f\"   Number of predictions: {len(forecast['forecasts'])}\")\n",
                "    print(f\"\\n   Sample prediction (Hour 0):\")\n",
                "    print(f\"   {forecast['forecasts'][0]}\")\n",
                "else:\n",
                "    print(f\"   ‚ùå Error: {response.json()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "footer"
            },
            "source": [
                "---\n",
                "\n",
                "## üìö Next Steps\n",
                "\n",
                "1. **Save the ngrok URL and API Key** from Step 6\n",
                "2. **Download the local client script** (`local_api_client.py`)\n",
                "3. **Update the client configuration** with your URL and API key\n",
                "4. **Run forecasts from your local machine!**\n",
                "\n",
                "### Keeping the API Running\n",
                "- The API will stay active as long as this Colab notebook is running\n",
                "- Free Colab sessions timeout after ~12 hours of inactivity\n",
                "- Consider Colab Pro for longer sessions\n",
                "\n",
                "### Troubleshooting\n",
                "- If the tunnel stops working, just re-run Step 6\n",
                "- Make sure to update your local client with the new URL and API key\n",
                "\n",
                "---\n",
                "\n",
                "**Built with ‚ù§Ô∏è using NASA IMPACT's Surya Foundation Model**"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}